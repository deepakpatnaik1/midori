# Learnings Log

This file contains learnings from development attempts. It's excluded from git to avoid conflicts with code changes.

---

## Phase 2, Chunk 2.1 - Create AppleSpeechManager

**Status**: 5 attempts completed. Attempt #5 achieved FULL SUCCESS and committed.

---

### Attempt #1 (FAILED)
**Date**: 2025-11-17
**What I did**: Created AppleSpeechManager.swift without reading existing code first

**Critical Bugs Found**:
1. **Wrong error type**: Used `Result<String, Error>` instead of `Result<String, TranscriptionError>`
2. **Hardcoded audio format**: Assumed 48kHz without verification
3. **Memory leaks**: No cleanup of recognitionRequest/recognitionTask
4. **Race conditions**: No thread-safe completion tracking

**Root Cause**: Didn't read TranscriptionManager.swift to understand existing interface before implementing

**How Fixed**: User asked me to abandon work and review with "fierce independence"

---

### Attempt #2 (FAILED)
**Date**: 2025-11-17
**What I did differently**: Read TranscriptionManager.swift first, then implemented AppleSpeechManager

**Critical Bug Found**:
1. **Duplicate enum definition**: Defined `TranscriptionError` enum in AppleSpeechManager.swift when it already exists in TranscriptionManager.swift

**Root Cause**: Didn't grep to check if TranscriptionError type already existed in codebase

**How Fixed**: User asked me to document learnings and abandon work

**Key Learning**: Before defining any type, grep to see if it already exists

---

### Attempt #3 (FAILED)
**Date**: 2025-11-17
**What I did differently**: Grepped for "TranscriptionError" to check if it exists, found it does, so didn't redefine it

**Critical Bug Found**:
1. **Non-existent enum cases**: Used `.speechRecognitionNotAuthorized` and `.speechRecognitionNotAvailable` which don't exist in the actual TranscriptionError enum

**Root Cause**: Found that TranscriptionError exists, but didn't read what cases it ACTUALLY contains. Assumed cases that seemed logical.

**How Fixed**: User asked me to document learnings and abandon work

**Key Learning**: When a type exists, don't just check that it exists - grep with `output_mode: "content"` and `-A 20` to see its ACTUAL definition and available cases

---

### Attempt #4 (SUCCESS with limitations)
**Date**: 2025-11-17
**What I did differently**:
1. Grepped for TranscriptionError with `-A 20` to see actual enum cases
2. Only used existing cases: `.noAudioData`, `.transcriptionFailed`, `.audioConversionFailed`
3. Mapped Apple Speech errors to these existing cases

**Implementation**:
- ‚úÖ Compiles successfully
- ‚úÖ Interface-compatible with TranscriptionManager (same signature)
- ‚úÖ Thread-safe completion tracking with DispatchQueue
- ‚úÖ Safe memory operations using memcpy
- ‚úÖ Proper cleanup of resources
- ‚úÖ Maps authorization errors to `.transcriptionFailed`
- ‚úÖ Maps recognizer unavailable to `.transcriptionFailed`

**Known Limitations** (accepted for Chunk 2.1 scope):
1. **48kHz hardcoded**: Matches TranscriptionManager's same assumption at line 135. This is an architectural limitation affecting both managers.
2. **Info.plist requirement**: Needs `NSSpeechRecognitionUsageDescription` key for App Store submission
3. **Inefficient double copy**: Extracts floatSamples array, then memcpy into buffer (could be optimized)
4. **Unnecessary auth callback**: Authorization request in init() only logs, doesn't add functionality

**User Feedback**: Asked me to document learnings, abandon work, and explain what I'd do differently for Attempt #5

---

### Attempt #5 (FULL SUCCESS - COMMITTED)
**Date**: 2025-11-17
**What I did differently**:
1. Applied all learnings from Attempts #1-4
2. Eliminated inefficient double array copy (direct memcpy from audioData)
3. Removed unnecessary authorization callback from init()
4. Added Info.plist requirement comment at top of file
5. Documented 48kHz limitation with reference to TranscriptionManager:135
6. Added TODO for future sample rate detection work

**Implementation**:
- ‚úÖ Compiles successfully
- ‚úÖ Interface-compatible with TranscriptionManager (drop-in replacement)
- ‚úÖ Thread-safe completion tracking with DispatchQueue
- ‚úÖ Safe memory operations using direct memcpy (no intermediate array)
- ‚úÖ Proper cleanup of resources
- ‚úÖ Maps all errors to existing TranscriptionError cases only
- ‚úÖ Comprehensive documentation of limitations and requirements
- ‚úÖ Cleaner code than Attempt #4

**Improvements over Attempt #4**:
1. More efficient memory usage (eliminated intermediate array allocation)
2. Cleaner init() without unnecessary authorization callback
3. Better documentation for future maintainers
4. Explicit TODO for architectural improvements

**User Feedback**: "Well done, you learned from your failures!!!" - Committed successfully

**Key Success Factors**:
1. Applied systematic approach from learnings
2. Read existing code first (TranscriptionManager.swift)
3. Grepped for types and verified actual enum cases with -A flag
4. Only used existing error cases
5. Focused improvements within scope (Chunk 2.1)
6. Documented known limitations clearly

---

## Key Patterns to Remember

### 1. Before implementing, READ existing code
- Read files that define interfaces you need to match
- Understand exact signatures, error types, data formats

### 2. Before defining any type, CHECK if it exists
- Grep for the type name
- Use `output_mode: "content"` with `-A 20` to see its ACTUAL definition
- Read what cases/methods/fields it actually has

### 3. Map to existing types, don't create new ones
- If an error type exists with specific cases, map your errors to those cases
- Don't assume new cases exist just because they seem logical
- Verify every case you use actually exists

### 4. Memory operations
- Use `memcpy` for safe memory copying
- Don't use `initialize()` on potentially initialized memory

### 5. Thread safety
- Use DispatchQueue for completion tracking
- Prevent multiple completion callbacks with flags

### 6. Workflow for abandoning work but preserving learnings
- Document learnings in `.learnings.md` (this file)
- Delete new code files manually
- Use `git restore` on code files as needed
- `.learnings.md` is in `.gitignore` so it's safe from git operations

---

## New Requirements / Issues Found

### Transcription Text Formatting
**Issue**: Transcribed text doesn't end with period and space
**Current behavior**: "Hello can you hear me" (no punctuation)
**Desired behavior**: "Hello can you hear me. " (period + space at end)

**Where to fix**:
- In `midoriApp.swift` around line 264 in `injectText()` method
- Currently adds space after text: `let textWithSpace = text + " "`
- Should add period and space: `let textWithSpace = text + ". "`

**Priority**: Medium (quality of life improvement)
**Phase**: Could be done in Phase 3 (UX improvements) or as quick fix now

**User feedback**: "At the end of every sentence I want there to be a full stop and a space programmatically added. Currently this is not happening I am having to manually add a period on a space"

---

## Phase 4, Custom Dictionary Implementation - FAILED

**Date**: 2025-11-17
**Status**: ABANDONED - Critical architecture flaw discovered during verification

### What I Built

Created complete infrastructure for custom vocabulary training:
1. **AppleSpeechManager.swift** - Apple Speech Framework wrapper for second-layer correction
2. **CustomLanguageModelManager.swift** - Training data persistence and model building
3. **TrainingWindow.swift** + **TrainingView.swift** - Full training UI
4. **Menu integration** - "Train Vocabulary" menu item (Cmd+T)
5. **Auto-loading** - Loads existing models on startup

All files compiled successfully. Build passed. UI opens.

### Critical Flaw Discovered

**The gas pedal is not connected to the engine.**

During fierce independent verification, I discovered AppleSpeechManager is NEVER invoked in the transcription flow:

**Current flow**:
```
User speaks ‚Üí AudioRecorder ‚Üí TranscriptionManager (Parakeet) ‚Üí injectText()
```

**What I built**:
```
AppleSpeechManager (sitting idle, never called)
CustomLanguageModelManager (stores training data, but no one uses it)
```

**What was needed**:
```
User speaks ‚Üí Parakeet ‚Üí IF custom model exists ‚Üí AppleSpeechManager ‚Üí corrected text ‚Üí injectText()
```

### Why This Failed

1. **Jumped ahead without completing dependencies** - Phase 4 requires Phase 2 (replace transcription engine) to be complete first per Implementation-Plan.md
2. **Built infrastructure without integration** - Created all the pieces but never wired them together
3. **Didn't verify end-to-end** - Built it, compiled it, but didn't trace the actual data flow
4. **Ignored the plan** - Implementation-Plan.md clearly states Phase 4 depends on Phase 2

### What Actually Needs To Happen

To make custom vocabulary work, three integration points are needed:

1. **Transcription flow modification** (midoriApp.swift:256-266):
```swift
// After Parakeet transcribes
case .success(let parakeetText):
    // If custom model exists, use Apple Speech as correction layer
    if self.customLanguageModelManager?.hasTrainedModel() == true,
       let audioData = self.audioRecorder?.getAudioData() {
        self.appleSpeechManager?.transcribe(audioData: audioData) { result in
            switch result {
            case .success(let correctedText):
                self.injectText(correctedText)
            case .failure:
                self.injectText(parakeetText) // Fallback to Parakeet
            }
        }
    } else {
        self.injectText(parakeetText)
    }
```

2. **Audio recording in training UI** - Need to connect AudioRecorder to TrainingView so users can actually record samples

3. **Proper language model API** - Current implementation uses simplified vocabulary file because `SFCustomLanguageModelData.Builder` doesn't exist in macOS 14

### Lessons Learned

1. **Always verify end-to-end** - Building pieces that compile doesn't mean they work
2. **Follow the dependency graph** - The Implementation Plan exists for a reason
3. **Trace the data flow** - Ask "where does this get called?" for every new component
4. **Integration is not optional** - Infrastructure without integration is worthless
5. **Verify with fierce independence** - Don't assume it works because it compiles

### User Feedback

User asked to "verify your work with fierce independence" - which revealed this fundamental flaw before wasting time debugging why training "doesn't work"

### Action Taken

Documenting learnings and reverting to stable commit before this work.

---

## Phase 4, Custom Dictionary Implementation - ATTEMPT #2 PLAN

**Date**: 2025-11-17
**Status**: PLANNING

### What I Will Do Differently

#### 1. Follow the Dependency Chain Strictly
- **FIRST**: Complete Phase 2 (Replace Transcription Engine)
- **THEN**: Do Phase 4 (Custom Dictionary)
- Implementation-Plan.md line 290-293 explicitly states Phase 4 requires Phase 2

#### 2. Test Integration Points FIRST, Infrastructure LAST
Instead of building all infrastructure then realizing it's not connected:

**Old approach (FAILED)**:
```
Build AppleSpeechManager ‚Üí Build CustomLanguageModelManager ‚Üí Build UI ‚Üí Oh no, nothing is wired up
```

**New approach**:
```
1. Wire up basic transcription flow with Apple Speech (no custom vocab yet)
2. Verify end-to-end: speak ‚Üí Apple transcribes ‚Üí text injected
3. ONLY THEN add custom vocabulary training infrastructure
```

#### 3. Verify After Each Integration Point
After each change, verify the complete data flow works:

**Step 1**: Replace Parakeet with Apple Speech (Phase 2)
- Verify: User speaks ‚Üí Apple Speech transcribes ‚Üí text appears ‚úì

**Step 2**: Add fallback to Parakeet if Apple Speech fails
- Verify: Both paths work ‚úì

**Step 3**: Add custom vocabulary (Phase 4)
- Verify: Training UI ‚Üí model builds ‚Üí custom words recognized ‚úì

#### 4. Implementation Order (Correct This Time)

**Phase 2 First** (Required dependency):
1. Read TranscriptionManager.swift to understand current interface
2. Create AppleSpeechManager with SAME interface as TranscriptionManager
3. Modify midoriApp.swift:256-266 to use AppleSpeechManager
4. Test: Does basic transcription work? (no custom vocab yet)
5. Commit when working

**Then Phase 4** (Only after Phase 2 works):
1. Add CustomLanguageModelManager
2. Add "IF model exists, load it" logic to AppleSpeechManager
3. Test: Does transcription still work without custom vocab? ‚úì
4. Add training UI
5. Test: Can I train a word and have it recognized? ‚úì
6. Commit when working

#### 5. Key Questions to Ask at Each Step

Before writing ANY code:
- **Where does this get called?** (Trace the data flow)
- **What data goes in?** (Match existing interfaces)
- **What data comes out?** (Match existing interfaces)
- **How do I test this works?** (Define verification steps)

After writing code:
- **Did I run it?** (Not just compile - actually use it)
- **Did I trace the execution?** (Verify the code path is hit)
- **Does the data flow work?** (Check logs/print statements)

#### 6. What Success Looks Like

**Phase 2 Success Criteria**:
- [ ] User speaks ‚Üí AppleSpeechManager.transcribe() is called
- [ ] Result appears as text (can see it on screen)
- [ ] No custom vocabulary yet, that's fine
- [ ] Commit message: "Phase 2 Complete: Switched to Apple Speech"

**Phase 4 Success Criteria** (only start after Phase 2 ‚úì):
- [ ] Open Training UI from menu
- [ ] Add phrase "Claude"
- [ ] Record audio sample (if possible - may need Phase 4.5)
- [ ] Set ground truth "Claude"
- [ ] Build model
- [ ] Speak "Claude" ‚Üí see "Claude" appear (not "clawed" or "clod")

#### 7. APIs to Verify BEFORE Using

Before implementing Phase 2, check:
- [ ] Does `SFSpeechRecognizer` exist in macOS 14?
- [ ] What's the API for audio buffer recognition?
- [ ] Can I actually use custom language models in macOS 14?
- [ ] If not, what's the fallback strategy?

#### 8. Acceptance Criteria for "Done"

Not done until:
1. User can speak ‚Üí see transcribed text
2. User can train word ‚Üí speak word ‚Üí see correct text
3. Persists across app restarts
4. No crashes, no silent failures

If ANY of these fail, integration is incomplete.

### Next Action - CONCRETE STEPS

**What I'll do differently in practice:**

#### Step-by-Step with Verification

**Step 1: Read FIRST, understand the interface**
```
Read TranscriptionManager.swift completely
Grep for: func transcribe
Verify: What parameters? What return type? What errors?
Write down: Input = Data, Output = Result<String, TranscriptionError>
```

**Step 2: Wire integration point BEFORE writing AppleSpeechManager**
```
Open midoriApp.swift:256
Comment out Parakeet line temporarily
Add print statement: "üîç TRANSCRIPTION FLOW HIT"
Run app, speak, verify print statement appears in logs
This proves I found the right integration point
```

**Step 3: Create MINIMAL AppleSpeechManager (just interface, no logic)**
```swift
class AppleSpeechManager {
    func transcribe(audioData: Data, completion: @escaping (Result<String, TranscriptionError>) -> Void) {
        print("üîç APPLESPEECHMANAGER CALLED")
        completion(.success("TEST TEXT"))
    }
}
```
Wire this into midoriApp.swift where Parakeet was
Run app, speak, verify:
1. "üîç APPLESPEECHMANAGER CALLED" appears in logs ‚úì
2. "TEST TEXT" appears on screen ‚úì

If EITHER fails ‚Üí integration is wrong, fix before proceeding

**Step 4: Add REAL Apple Speech logic (still no custom vocab)**
```
Replace "TEST TEXT" with actual SFSpeechRecognizer code
Run app, speak actual words
Verify: My words appear on screen ‚úì
```

**Step 5: ONLY THEN commit Phase 2**
```
git add -A
git commit -m "Phase 2 Complete: Apple Speech integrated and working"
```

**Step 6: Add custom vocabulary support (Phase 4)**
```
Add CustomLanguageModelManager
Add loadModel logic to AppleSpeechManager
Test: Does basic transcription STILL work without model? ‚úì
Add Training UI
Test: Can I train "Claude" and have it recognized? ‚úì
```

#### The Critical Difference

**Attempt #1 (WRONG)**:
- Wrote 600 lines of code
- Never added print statements to verify call path
- Assumed it worked because it compiled
- Discovered at end that nothing was connected

**Attempt #2 (RIGHT)**:
- Write 5 lines of code
- Add print statement
- Run app, speak, check logs
- See print statement or STOP and fix
- Only proceed when verified working

#### Verification Checklist (MANDATORY after each step)

After EVERY code change:
- [ ] Did I add a print statement showing this code runs?
- [ ] Did I restart the app?
- [ ] Did I speak into the microphone?
- [ ] Did I check the logs?
- [ ] Did I see my print statement?
- [ ] Did I see the expected result?

If ANY checkbox is unchecked ‚Üí integration is incomplete

#### What "Test end-to-end" Actually Means

Not just "does it compile" but:
1. Kill existing Midori process
2. Build and install: `./scripts/build.sh && ./scripts/install-local.sh`
3. Open new Midori instance
4. Press right command key
5. Wait for double pop
6. Speak: "Hello can you hear me"
7. Release right command key
8. Check: Did "Hello can you hear me" appear on screen?
9. Check logs: `log show --predicate 'process == "midori"' --last 1m | grep "üîç"`

If text doesn't appear OR print statement missing ‚Üí IT DOESN'T WORK

---
